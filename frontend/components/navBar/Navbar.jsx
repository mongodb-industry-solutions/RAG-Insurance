"use client";

import UserProfile from "../userProfile/UserProfile";
import styles from "./navbar.module.css";
import Image from "next/image";
import { useState } from "react";
import InfoWizard from "../InfoWizard/InfoWizard";

const Navbar = () => {
  const [openHelpModal, setOpenHelpModal] = useState(false);
  return (
    <nav className={styles.navbar}>
      <div className={styles.logo}>
        <Image src="/assets/logo.png" alt="Logo" width={200} height={40} />{" "}
      </div>
      <InfoWizard
          open={openHelpModal}
          setOpen={setOpenHelpModal}
          tooltipText="Tell me more!"
          iconGlyph="Wizard"
          sections={[
            {
              heading: "Instructions and Talk Track",
              content: [
                {
                  heading: "RAG for claim processing",
                  body: "integrating Atlas Vector Search and LLMs, thus allowing insurers to go beyond the limitations of baseline foundational models, making them context-aware by feeding them proprietary data. Through a chat prompt, we can ask questions to the system, and the LLM returns answers to the user and shows what references it used to retrieve the information contained in the response.",
                },
                {
                  heading: "How to Demo",
                  body: [
                    "Type a question in the box or use the suggested ones",
                    "Press 'Ask'",  
                    "On the right you can see the retrieved documents. Please note that the 'Claim description' field was the one indexed and used for the vector search",
                    "On the left the LLM’s output is generated by taking into account the documents on the right. This is a standard RAG application"
                  ],
                },
              ],
            },
            {
              heading: "Behind the Scenes",
              content: [
                {
                  heading: "Data Flow",
                  body: "",
                },
                {
                  image: {
                    src: "assets/architecture.png",
                    alt: "Architecture",
                  },
                },
              ],
            },
            {
              heading: "Why MongoDB?",
              content: [
                {
                  heading: "The benefits of combining Atlas Vector Search and LLMs in a Claim Processing RAG application:",
                  body: [
                        "Speed and accuracy: Having the data centrally organized and ready to be consumed by LLMs, adjusters can find all the necessary information in a fraction of the time.",
                        "Flexibility: LLMs can answer a wide spectrum of questions, meaning applications require less upfront system design. There is no need to build custom APIs for each piece of information you’re trying to retrieve; just ask the LLM to do it for you.  ",
                        "Natural interaction: Applications can be interrogated in plain English without programming skills or system training.  ", 
                        "Data accessibility: Insurers can finally leverage and explore unstructured data that was previously hard to access.." ]
                },
              ],
            },
          ]}
        />
      <div className={styles.user}>
        <UserProfile />
      </div>
    </nav>
  );
};

export default Navbar;
